{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee304748-ee59-4b5e-8fac-51607f30f6d7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Tokenization\n",
    "\n",
    "* Tokenization is a crucial process in natural language processing (NLP) where text is broken down into smaller units called tokens. \n",
    "* These tokens can be words, subwords, characters, or other meaningful elements depending on the tokenization strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88c65bd8-574f-4342-99f0-fd40cc0a7464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9802d8-a0b9-4ecf-827f-ca1e4fb82351",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52f69052-6a49-4a56-b6c3-4feea23d7e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence tokenizer: \n",
      " ['\\nTokenization is a crucial process in natural language processing (NLP) where text is broken down into smaller units called tokens.', 'These tokens can be words, subwords, characters, or other meaningful elements depending on the tokenization strategy.', 'Tokenization is a fundamental step in preparing text data for machine learning models, particularly for tasks like text classification, language modeling, translation, and sentiment analysis.']\n",
      "word tokenizer: \n",
      " ['Tokenization', 'is', 'a', 'crucial', 'process', 'in', 'natural', 'language', 'processing', '(', 'NLP', ')', 'where', 'text', 'is', 'broken', 'down', 'into', 'smaller', 'units', 'called', 'tokens', '.', 'These', 'tokens', 'can', 'be', 'words', ',', 'subwords', ',', 'characters', ',', 'or', 'other', 'meaningful', 'elements', 'depending', 'on', 'the', 'tokenization', 'strategy', '.', 'Tokenization', 'is', 'a', 'fundamental', 'step', 'in', 'preparing', 'text', 'data', 'for', 'machine', 'learning', 'models', ',', 'particularly', 'for', 'tasks', 'like', 'text', 'classification', ',', 'language', 'modeling', ',', 'translation', ',', 'and', 'sentiment', 'analysis', '.']\n"
     ]
    }
   ],
   "source": [
    "paragraph = \"\"\"\n",
    "Tokenization is a crucial process in natural language processing (NLP) where text is broken down into smaller units called tokens. These tokens can be words, subwords, characters, or other meaningful elements depending on the tokenization strategy. Tokenization is a fundamental step in preparing text data for machine learning models, particularly for tasks like text classification, language modeling, translation, and sentiment analysis.\n",
    "\"\"\"\n",
    "\n",
    "sentence = nltk.sent_tokenize(paragraph)\n",
    "print(\"sentence tokenizer: \\n\", sentence)\n",
    "\n",
    "word = nltk.word_tokenize(paragraph)\n",
    "print(\"word tokenizer: \\n\", word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c98a13-012e-4e1d-9aaf-955b487bac56",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Stemming\n",
    "\n",
    "* Stemming is a text normalization technique in natural language processing (NLP) that reduces words to their root or base form, called the stem, by removing affixes (suffixes or prefixes). \n",
    "* The stem may not necessarily be a linguistically valid word.\n",
    "* It is faster due to simple rules.\n",
    "* Applications: search engines, sentiment analysis, text mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabbcf2f-3fb3-4f60-8bd2-cf56ba16cb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1bcd5154-ad83-4032-9d3d-30d4bdd6eeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3058988-a087-4a72-898e-9ddb380cb6a0",
   "metadata": {},
   "source": [
    "**Porter Stemmer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "910f997c-81c1-41ea-af74-3ca48052d8e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence tokenizer: \n",
      " ['\\nTokenization is a crucial process in natural language processing (NLP) where text is broken down into smaller units called tokens.', 'These tokens can be words, subwords, characters, or other meaningful elements depending on the tokenization strategy.', 'Tokenization is a fundamental step in preparing text data for machine learning models, particularly for tasks like text classification, language modeling, translation, and sentiment analysis.']\n",
      "After stemming: \n",
      " ['token crucial process natur languag process ( nlp ) text broken smaller unit call token .', 'these token word , subword , charact , meaning element depend token strategi .', 'token fundament step prepar text data machin learn model , particularli task like text classif , languag model , translat , sentiment analysi .']\n"
     ]
    }
   ],
   "source": [
    "paragraph = \"\"\"\n",
    "Tokenization is a crucial process in natural language processing (NLP) where text is broken down into smaller units called tokens. These tokens can be words, subwords, characters, or other meaningful elements depending on the tokenization strategy. Tokenization is a fundamental step in preparing text data for machine learning models, particularly for tasks like text classification, language modeling, translation, and sentiment analysis.\n",
    "\"\"\"\n",
    "\n",
    "sentence = nltk.sent_tokenize(paragraph)\n",
    "print(\"sentence tokenizer: \\n\", sentence)\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "for i in range(len(sentence)):\n",
    "    words = nltk.word_tokenize(sentence[i])\n",
    "    words = [stemmer.stem(word) for word in words if word not in stopwords.words('english')]\n",
    "    sentence[i] = \" \".join(words)\n",
    "print(\"After stemming: \\n\", sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7e24d9-1fc1-4d42-8c99-180bd4ca39d0",
   "metadata": {},
   "source": [
    "**Snowball Stemmer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a47ee48-adcd-4632-86eb-240c3e0287bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "072f8418-b909-453e-acac-80ac72917d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence tokenizer: \n",
      " ['\\nTokenization is a crucial process in natural language processing (NLP) where text is broken down into smaller units called tokens.', 'These tokens can be words, subwords, characters, or other meaningful elements depending on the tokenization strategy.', 'Tokenization is a fundamental step in preparing text data for machine learning models, particularly for tasks like text classification, language modeling, translation, and sentiment analysis.']\n",
      "After stemming: \n",
      " ['token crucial process natur languag process ( nlp ) text broken smaller unit call token .', 'these token word , subword , charact , meaning element depend token strategi .', 'token fundament step prepar text data machin learn model , particular task like text classif , languag model , translat , sentiment analysi .']\n"
     ]
    }
   ],
   "source": [
    "paragraph = \"\"\"\n",
    "Tokenization is a crucial process in natural language processing (NLP) where text is broken down into smaller units called tokens. These tokens can be words, subwords, characters, or other meaningful elements depending on the tokenization strategy. Tokenization is a fundamental step in preparing text data for machine learning models, particularly for tasks like text classification, language modeling, translation, and sentiment analysis.\n",
    "\"\"\"\n",
    "\n",
    "sentence = nltk.sent_tokenize(paragraph)\n",
    "print(\"sentence tokenizer: \\n\", sentence)\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "for i in range(len(sentence)):\n",
    "    words = nltk.word_tokenize(sentence[i])\n",
    "    words = [stemmer.stem(word) for word in words if word not in stopwords.words('english')]\n",
    "    sentence[i] = \" \".join(words)\n",
    "print(\"After stemming: \\n\", sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623f3a3e-4967-470a-9281-dfaf8717d86f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Lemmatization\n",
    "\n",
    "* Lemmatization is a text normalization technique in natural language processing (NLP) that reduces a word to its lemma, which is its dictionary form or base meaning. \n",
    "* Unlike stemming, lemmatization considers the context and part of speech (POS) of a word to produce linguistically valid outputs.\n",
    "* It is slower due to complex processing.\n",
    "* Depends on dictionary and morphological rules to output valid word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a1d5fa34-e56b-4976-8f3e-9ab458e533af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\pooja\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\pooja\\AppData\\Roaming\\nltk_data...\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bb03bdab-4572-4c96-aa66-3a19c4c1f5cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence tokenizer: \n",
      " ['\\nTokenization is a crucial process in natural language processing (NLP) where text is broken down into smaller units called tokens.', 'These tokens can be words, subwords, characters, or other meaningful elements depending on the tokenization strategy.', 'Tokenization is a fundamental step in preparing text data for machine learning models, particularly for tasks like text classification, language modeling, translation, and sentiment analysis.']\n",
      "After stemming: \n",
      " ['Tokenization crucial process natural language processing ( NLP ) text broken smaller unit called token .', 'These token word , subwords , character , meaningful element depending tokenization strategy .', 'Tokenization fundamental step preparing text data machine learning model , particularly task like text classification , language modeling , translation , sentiment analysis .']\n"
     ]
    }
   ],
   "source": [
    "paragraph = \"\"\"\n",
    "Tokenization is a crucial process in natural language processing (NLP) where text is broken down into smaller units called tokens. These tokens can be words, subwords, characters, or other meaningful elements depending on the tokenization strategy. Tokenization is a fundamental step in preparing text data for machine learning models, particularly for tasks like text classification, language modeling, translation, and sentiment analysis.\n",
    "\"\"\"\n",
    "\n",
    "sentence = nltk.sent_tokenize(paragraph)\n",
    "print(\"sentence tokenizer: \\n\", sentence)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "for i in range(len(sentence)):\n",
    "    words = nltk.word_tokenize(sentence[i])\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in stopwords.words('english')]\n",
    "    sentence[i] = \" \".join(words)\n",
    "print(\"After stemming: \\n\", sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5c8e65-c5e1-4a85-9240-e3b7b69a910b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Bag of Words\n",
    "\n",
    "* The Bag of Words (BoW) transforms text into a numerical format that machine learning models can process by considering the occurrence of words, without paying attention to grammar, word order, or context.\n",
    "* BOW is the special case of n-gram, where n=1\n",
    "* In the context of n-grams, a unigram is an n-gram with n=1, meaning it considers only individual words without looking at their surrounding words or context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f8776b40-090a-453c-8525-975c5375a3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ba88130d-da9e-4f79-831e-682ced471df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence tokenizer: \n",
      " ['\\nTokenization is a crucial process in natural language processing (NLP) where text is broken down into smaller units called tokens.', 'These tokens can be words, subwords, characters, or other meaningful elements depending on the tokenization strategy.', 'Tokenization is a fundamental step in preparing text data for machine learning models, particularly for tasks like text classification, language modeling, translation, and sentiment analysis.']\n",
      "corpus: \n",
      " ['tokenization crucial process natural language processing (nlp) text broken smaller unit called tokens.', 'hese token words, subwords, characters, meaningful element depending tokenization strategy.', 'okenization fundamental step preparing text data machine learning models, particularly task like text classification, language modeling, translation, sentiment analysis.']\n",
      "Features:  ['analysis' 'broken' 'called' 'characters' 'classification' 'crucial'\n",
      " 'data' 'depending' 'element' 'fundamental' 'hese' 'language' 'learning'\n",
      " 'like' 'machine' 'meaningful' 'modeling' 'models' 'natural' 'nlp'\n",
      " 'okenization' 'particularly' 'preparing' 'process' 'processing'\n",
      " 'sentiment' 'smaller' 'step' 'strategy' 'subwords' 'task' 'text' 'token'\n",
      " 'tokenization' 'tokens' 'translation' 'unit' 'words']\n",
      "****************************************************************************************************\n",
      "Vocabulary: {'tokenization': 33, 'crucial': 5, 'process': 23, 'natural': 18, 'language': 11, 'processing': 24, 'nlp': 19, 'text': 31, 'broken': 1, 'smaller': 26, 'unit': 36, 'called': 2, 'tokens': 34, 'hese': 10, 'token': 32, 'words': 37, 'subwords': 29, 'characters': 3, 'meaningful': 15, 'element': 8, 'depending': 7, 'strategy': 28, 'okenization': 20, 'fundamental': 9, 'step': 27, 'preparing': 22, 'data': 6, 'machine': 14, 'learning': 12, 'models': 17, 'particularly': 21, 'task': 30, 'like': 13, 'classification': 4, 'modeling': 16, 'translation': 35, 'sentiment': 25, 'analysis': 0}\n",
      "****************************************************************************************************\n",
      "Bag of Words: \n",
      " [[0 1 1 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 1 1 0 1 0 0 0 0 1 0 1 1 0\n",
      "  1 0]\n",
      " [0 0 0 1 0 0 0 1 1 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 1 0 0\n",
      "  0 1]\n",
      " [1 0 0 0 1 0 1 0 0 1 0 1 1 1 1 0 1 1 0 0 1 1 1 0 0 1 0 1 0 0 1 2 0 0 0 1\n",
      "  0 0]]\n"
     ]
    }
   ],
   "source": [
    "paragraph = \"\"\"\n",
    "Tokenization is a crucial process in natural language processing (NLP) where text is broken down into smaller units called tokens. These tokens can be words, subwords, characters, or other meaningful elements depending on the tokenization strategy. Tokenization is a fundamental step in preparing text data for machine learning models, particularly for tasks like text classification, language modeling, translation, and sentiment analysis.\n",
    "\"\"\"\n",
    "# text cleaning\n",
    "sentence = nltk.sent_tokenize(paragraph)\n",
    "print(\"sentence tokenizer: \\n\", sentence)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "corpus = []\n",
    "for i in range(len(sentence)):\n",
    "    review = re.sub('^[a-zA-Z]', \"\", sentence[i])\n",
    "    review = review.lower().split()\n",
    "    review = [lemmatizer.lemmatize(word) for word in review if word not in stopwords.words('english')]\n",
    "    sentence[i] = \" \".join(review)\n",
    "    corpus.append(sentence[i])\n",
    "print(\"corpus: \\n\", corpus)\n",
    "\n",
    "# vectorization\n",
    "cv = CountVectorizer()\n",
    "output = cv.fit_transform(corpus).toarray()\n",
    "print(\"Features: \", cv.get_feature_names_out())\n",
    "print(\"*\"*100)\n",
    "print(\"Vocabulary:\", cv.vocabulary_)\n",
    "print(\"*\"*100)\n",
    "print(\"Bag of Words: \\n\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fa8c6da7-c8b9-4558-b044-f38c42a997ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  ['broken smaller' 'called tokens' 'characters meaningful'\n",
      " 'classification language' 'crucial process' 'data machine'\n",
      " 'depending tokenization' 'element depending' 'fundamental step'\n",
      " 'hese token' 'language modeling' 'language processing' 'learning models'\n",
      " 'like text' 'machine learning' 'meaningful element'\n",
      " 'modeling translation' 'models particularly' 'natural language'\n",
      " 'nlp text' 'okenization fundamental' 'particularly task' 'preparing text'\n",
      " 'process natural' 'processing nlp' 'sentiment analysis' 'smaller unit'\n",
      " 'step preparing' 'subwords characters' 'task like' 'text broken'\n",
      " 'text classification' 'text data' 'token words' 'tokenization crucial'\n",
      " 'tokenization strategy' 'translation sentiment' 'unit called'\n",
      " 'words subwords']\n",
      "****************************************************************************************************\n",
      "Vocabulary: {'tokenization crucial': 34, 'crucial process': 4, 'process natural': 23, 'natural language': 18, 'language processing': 11, 'processing nlp': 24, 'nlp text': 19, 'text broken': 30, 'broken smaller': 0, 'smaller unit': 26, 'unit called': 37, 'called tokens': 1, 'hese token': 9, 'token words': 33, 'words subwords': 38, 'subwords characters': 28, 'characters meaningful': 2, 'meaningful element': 15, 'element depending': 7, 'depending tokenization': 6, 'tokenization strategy': 35, 'okenization fundamental': 20, 'fundamental step': 8, 'step preparing': 27, 'preparing text': 22, 'text data': 32, 'data machine': 5, 'machine learning': 14, 'learning models': 12, 'models particularly': 17, 'particularly task': 21, 'task like': 29, 'like text': 13, 'text classification': 31, 'classification language': 3, 'language modeling': 10, 'modeling translation': 16, 'translation sentiment': 36, 'sentiment analysis': 25}\n",
      "****************************************************************************************************\n",
      "Bag of Words: \n",
      " [[1 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 1 1 0 1 0 0 0 1 0 0 0 1 0\n",
      "  0 1 0]\n",
      " [0 0 1 0 0 0 1 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1\n",
      "  0 0 1]\n",
      " [0 0 0 1 0 1 0 0 1 0 1 0 1 1 1 0 1 1 0 0 1 1 1 0 0 1 0 1 0 1 0 1 1 0 0 0\n",
      "  1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# bigram n=2\n",
    "cv = CountVectorizer(ngram_range=(2,2))\n",
    "output = cv.fit_transform(corpus).toarray()\n",
    "print(\"Features: \", cv.get_feature_names_out())\n",
    "print(\"*\"*100)\n",
    "print(\"Vocabulary:\", cv.vocabulary_)\n",
    "print(\"*\"*100)\n",
    "print(\"Bag of Words: \\n\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6067809-52eb-47b2-9b6a-f290c2460736",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# TF-IDF\n",
    "\n",
    "* A refinement of BoW that accounts for the importance of words in the entire corpus.\n",
    "\n",
    "Formula:\n",
    "TF-IDF = TF×IDF\n",
    "\n",
    "where:\n",
    "TF: Term frequency (how often a term appears in a document).\n",
    "\n",
    "IDF: Inverse document frequency (how unique the term is across documents).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ef29f2dc-1245-45f4-b165-14a6350fcac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "57f5557c-cfeb-4439-b8d1-77d2c868858d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence tokenizer: \n",
      " ['\\nTokenization is a crucial process in natural language processing (NLP) where text is broken down into smaller units called tokens.', 'These tokens can be words, subwords, characters, or other meaningful elements depending on the tokenization strategy.', 'Tokenization is a fundamental step in preparing text data for machine learning models, particularly for tasks like text classification, language modeling, translation, and sentiment analysis.']\n",
      "corpus: \n",
      " ['tokenization crucial process natural language processing (nlp) text broken smaller unit called tokens.', 'hese token words, subwords, characters, meaningful element depending tokenization strategy.', 'okenization fundamental step preparing text data machine learning models, particularly task like text classification, language modeling, translation, sentiment analysis.']\n",
      "Features:  ['analysis' 'broken' 'called' 'characters' 'classification' 'crucial'\n",
      " 'data' 'depending' 'element' 'fundamental' 'hese' 'language' 'learning'\n",
      " 'like' 'machine' 'meaningful' 'modeling' 'models' 'natural' 'nlp'\n",
      " 'okenization' 'particularly' 'preparing' 'process' 'processing'\n",
      " 'sentiment' 'smaller' 'step' 'strategy' 'subwords' 'task' 'text' 'token'\n",
      " 'tokenization' 'tokens' 'translation' 'unit' 'words'] 38\n",
      "****************************************************************************************************\n",
      "Vocabulary: {'tokenization': 33, 'crucial': 5, 'process': 23, 'natural': 18, 'language': 11, 'processing': 24, 'nlp': 19, 'text': 31, 'broken': 1, 'smaller': 26, 'unit': 36, 'called': 2, 'tokens': 34, 'hese': 10, 'token': 32, 'words': 37, 'subwords': 29, 'characters': 3, 'meaningful': 15, 'element': 8, 'depending': 7, 'strategy': 28, 'okenization': 20, 'fundamental': 9, 'step': 27, 'preparing': 22, 'data': 6, 'machine': 14, 'learning': 12, 'models': 17, 'particularly': 21, 'task': 30, 'like': 13, 'classification': 4, 'modeling': 16, 'translation': 35, 'sentiment': 25, 'analysis': 0}\n",
      "****************************************************************************************************\n",
      "TFIDF: \n",
      " [[0.         0.2919139  0.2919139  0.         0.         0.2919139\n",
      "  0.         0.         0.         0.         0.         0.22200805\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.2919139  0.2919139  0.         0.         0.         0.2919139\n",
      "  0.2919139  0.         0.2919139  0.         0.         0.\n",
      "  0.         0.22200805 0.         0.22200805 0.2919139  0.\n",
      "  0.2919139  0.        ]\n",
      " [0.         0.         0.         0.32311233 0.         0.\n",
      "  0.         0.32311233 0.32311233 0.         0.32311233 0.\n",
      "  0.         0.         0.         0.32311233 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.32311233 0.32311233\n",
      "  0.         0.         0.32311233 0.24573525 0.         0.\n",
      "  0.         0.32311233]\n",
      " [0.23007057 0.         0.         0.         0.23007057 0.\n",
      "  0.23007057 0.         0.         0.23007057 0.         0.1749746\n",
      "  0.23007057 0.23007057 0.23007057 0.         0.23007057 0.23007057\n",
      "  0.         0.         0.23007057 0.23007057 0.23007057 0.\n",
      "  0.         0.23007057 0.         0.23007057 0.         0.\n",
      "  0.23007057 0.34994919 0.         0.         0.         0.23007057\n",
      "  0.         0.        ]] (3, 38)\n",
      "****************************************************************************************************\n",
      "IDF: \n",
      " [1.69314718 1.69314718 1.69314718 1.69314718 1.69314718 1.69314718\n",
      " 1.69314718 1.69314718 1.69314718 1.69314718 1.69314718 1.28768207\n",
      " 1.69314718 1.69314718 1.69314718 1.69314718 1.69314718 1.69314718\n",
      " 1.69314718 1.69314718 1.69314718 1.69314718 1.69314718 1.69314718\n",
      " 1.69314718 1.69314718 1.69314718 1.69314718 1.69314718 1.69314718\n",
      " 1.69314718 1.28768207 1.69314718 1.28768207 1.69314718 1.69314718\n",
      " 1.69314718 1.69314718]\n"
     ]
    }
   ],
   "source": [
    "paragraph = \"\"\"\n",
    "Tokenization is a crucial process in natural language processing (NLP) where text is broken down into smaller units called tokens. These tokens can be words, subwords, characters, or other meaningful elements depending on the tokenization strategy. Tokenization is a fundamental step in preparing text data for machine learning models, particularly for tasks like text classification, language modeling, translation, and sentiment analysis.\n",
    "\"\"\"\n",
    "# text cleaning\n",
    "sentence = nltk.sent_tokenize(paragraph)\n",
    "print(\"sentence tokenizer: \\n\", sentence)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "corpus = []\n",
    "for i in range(len(sentence)):\n",
    "    review = re.sub('^[a-zA-Z]', \"\", sentence[i])\n",
    "    review = review.lower().split()\n",
    "    review = [lemmatizer.lemmatize(word) for word in review if word not in stopwords.words('english')]\n",
    "    sentence[i] = \" \".join(review)\n",
    "    corpus.append(sentence[i])\n",
    "print(\"corpus: \\n\", corpus)\n",
    "\n",
    "# vectorization\n",
    "cv = TfidfVectorizer()\n",
    "output = cv.fit_transform(corpus).toarray()\n",
    "print(\"Features: \", cv.get_feature_names_out(), len(cv.get_feature_names_out()))\n",
    "print(\"*\"*100)\n",
    "print(\"Vocabulary:\", cv.vocabulary_)\n",
    "print(\"*\"*100)\n",
    "print(\"TFIDF: \\n\", output, output.shape)\n",
    "print(\"*\"*100)\n",
    "print(\"IDF: \\n\", cv.idf_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7ae2b3-1bd2-4c4d-a88f-5c784360489a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
